---
title: "Low-Rank Adaptation(LoRA)的实现"
excerpt: "用于大模型微调的LoRA方法实现"
categories:
  - Fine tuning
---

## Low-Rank Adaptation(LoRA)的原理与实现

LoRA是一种参数高效微调方法，其核心思想是：冻结原模型参数，只通过在权重更新上引入低秩分解来完成微调，从而极大减少可训练参数与显存开销。常用于大模型微调。

### LoRA原理

LoRA是在原来模型的线性变换（例如卷积层、线性层）中，对权重做一个低秩扰动。LoRA的假设是：在参数空间中，从预训练模型到适配下游任务的模型的“位移”$\Delta W$存在于一个低维子空间中。因此通过在原来权重上加一个低秩矩阵来实现模型的微调适配。

LoRA的流程如图所示，在原来的权重$W_{d×k}$中加入两个低秩矩阵$A_{r×k}$和$B_{d×r}$，在Vanilla LoRA中，对$A$进行高斯初始化，对$B$进行零初始化。原论文中也讨论了互换初始化方法，但认为没有区别。不能同时零初始化，会导致梯度为零，无法更新。对其中一个零初始化是为了让初始前向结果为零。

![图源：https://huggingface.co/blog/zh/4bit-transformers-bitsandbytes](/assets/images/2026-1-28/lora-animated.gif)

### LoRA代码实现

#### LoRA线性层定义

```python
import torch
import torch.nn as nn
import torch.nn.functinal as F

class LoraLinear(nn.Module)
	def __init__(
    	self,
        base_layer: nn.Linear,		# 需要替换的目标线性层
        r: int = 8,					# LoRA的秩
        alpha: int = 16,			# 控制LoRA影响的系数
        dropout_p: float = 0.0,		# dropout概率
    ):
        super(LoraLinear, self).__init__()
        self.base_layer = copy.deepcopy(base_layer)	# 对目标线性层进行深拷贝
        self.r = r
        self.alpha = alpha
        self.dropout = nn.Dropout(dropout_p)
        
        # 定义两个低秩矩阵A和B，注意A和B的维度顺序
        self.lora_A = nn.Parameter(torch.empty((r, base_layer.in_features), dtype=base_layer.weight.dtype))
        self.lora_B = nn.Parameter(torch.empty((base_layer.out_features, r), dtype=base_layer.weight.dtype))
        
        # 初始化
        nn.init.normal_(self.lora_A, mean=0.0, std=0.02)
        nn.init.zeros_(self.lora_B)
        
        # 冻结目标线性层的权重
        for param in self.base_layer.parameters():
            param.requires_grad = False
            
	def forward(self, x: torch.Tensor) -> torch.Tensor:
        scaling = float(self.alpha) / float(self.r) # 控制LoRA影响的缩放系数
        # F.linear(input, weight, bias=None)	xW^T + b
        lora_adjustment = F.linear(self.dropout(x), self.lora_A)
        lora_adjustment = F.linear(lora_adjustment, self.lora_B)
        return self.base_layer(x) + scaling * lora_adjustment
```

#### LoRA层替换

```python
import torch.nn as nn

# 检索输入模块中的层，将所有线性变换加入LoRA
def replace_linear_with_lora(
	module: nn.Module,
    r: int = 8,
    alpha: int = 16,
    dropout_p: float = 0.0,
    embed_requires_grad: bool = False,
    norm_requires_grad: bool = False,
    head_requires_grad: bool = False,
):
    # named_children会返回名称和下一级的子模块
    for name, child in module.named_children():
        if any(s in name for s in ['embed', 'norm', 'lm_head']):
            requires_grad = embed_requires_grad if 'embed' in name \
            				else norm_requires_grad if 'norm' in name \
            				else head_requires_grad
            for param in child.parameters():
                param.requires_grad = requires_grad
        elif isinstance(child, nn.Linear):
            lora_linear = LoraLinear(child, r=r, alpha=alpha, doupout_p=droupout_p)
            # setattr用于给对象动态修改属性
            setattr(module, name, lora_linear)
        else:
            replace_linear_with_lora(
            	child, r, alpha, dropout_p,
                embed_requires_grad, norm_requires_grad, head_requires_grad,
            )
```

### 引用

· [动手实现 LoRA - LoRA from scratch——知乎](https://zhuanlan.zhihu.com/p/702419731)

· [LORA：大模型轻量级微调](https://www.zhihu.com/tardis/zm/art/623543497?source_id=1005)
