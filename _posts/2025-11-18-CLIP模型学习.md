---
title: "CLIP模型学习"
excerpt: "CLIP的模型结构与代码学习"
categories:
  - Vision-Language Model
---

论文标题：《[Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020)》【[GitHub](https://github.com/OpenAI/CLIP)】

## 论文动机与创新

视觉领域的传统监督模型在图像分类（例如ImageNet分类）上已有一定成效，但是这类任务形式仍然存在一些问题：

1. 人工标注的数据集获取成本高昂且分类类别有限，难以实现泛化，**而自然语言处理任务已经证明了大规模无监督/弱监督能够实现“跨任务泛化”**，从而减少对特定任务的数据集需求和训练。

2. 自然语言相比固定类别标签更加灵活和通用，传统的ImageNet只包含了1000个类别，而自然语言可以描述各种类别。但是之前使用自然语言作为监督信号的工作都没能取得较好性能结果，**作者认为导致这种情况的原因在于数据规模的不足**。

针对以上问题和限制，作者提出了**Contrastive Language-Image Pre-training（CLIP）**模型。该模型首次在大规模图文配对数据集上使用自然语言监督训练视觉模型，该模型具有零样本迁移的能力，并且可扩展性强（性能可以随着模型计算量上升而提高）。训练CLIP的图像-文本配对数据集是从互联网直接获取，而非人工精确标注，因此算偏向于弱监督的训练方式。

**补充：**
**监督学习（Supervised Learning）**是每个样本都有准确、完整的人工标注。一般来说成本高且标签集固定。
**弱监督学习（Weakly Supervised Learning）**是每个样本仍有标签，但是标签通常是信息不完整、不精确的。这类标签获取成本低，虽然不准确，但是可以扩充到大规模。
**无监督学习（Unsupervised Learning）**则是样本完全没有标签，让模型自己挖掘数据本身的结构特征与数据之间的关系。
{: .notice--info}

## 模型的结构与训练

![CLIP模型图](/assets/images/2025-11-18/0001.png)

CLIP的整体模型结构如图所示。作者的目标是引入自然语言作为训练的监督信号，但是如果要求模型能够将图片与某个特定词汇一一对应将需要更多的计算量，并且不够泛化灵活。因此作者选择让模型具有让图片与对应的描述文本具有更高的相似度即可。具体来说，使用**文本编码器（Text Encoder）**来对 N 段文本进行编码，每段文本对应一个向量，再使用**图像编码器（Image Encoder）**对 N 张图片进行编码，每张图片对应一个向量，然后计算文本向量和图片向量的两两余弦相似度，从而判断文本分别和哪些图片对应。

CLIP的伪代码如下所示。

```c
# image_encoder - ResNet or Vision Transformer
# text_encoder - CBOW or Text Transformer
# I[n, h, w, c] - minibatch of aligned images
# T[n, l] - minibatch of aligned texts
# W_i[d_i, d_e] - learned proj of image to embed
# W_t[d_t, d_e] - learned proj of text to embed
# t - learned temperature parameter

# extract feature representations of each modality
I_f = image_encoder(I) #[n, d_i]
T_f = text_encoder(T) #[n, d_t]

# joint multimodal embedding [n, d_e]
I_e = l2_normalize(np.dot(I_f, W_i), axis=1)
T_e = l2_normalize(np.dot(T_f, W_t), axis=1)

# scaled pairwise cosine similarities [n, n]
logits = np.dot(I_e, T_e.T) * np.exp(t)

# symmetric loss function
labels = np.arange(n)
loss_i = cross_entropy_loss(logits, labels, axis=0)
loss_t = cross_entropy_loss(logits, labels, axis=1)
loss = (loss_i + loss_t)/2
```

**对于图像编码器**，作者选取了**5种ResNet模型**和**3种Vision Transformer模型**。前者包含了ResNet-50、ResNet-101，以及采用从多维度扩展模型ResNet-50得到的RN50×4、RN50×16和RN50×64；后者包含了ViT-B/32、ViT-B/16和ViT-L/14。其中对于ResNet进行了部分结构修改：将全局平均池化替换为注意力池化机制，采用了ResNet-D改进和抗锯齿模糊池化。对于ViT则在位置编码处额外加入一层Layer Normalization。

**对于文本编码器**，作者采用了Transformer，使用了掩码自注意力机制。对输入文本的开头和结尾加上[SOS]和[EOS]，最后的编码输出中[EOS]处的向量作为整条文本的整体特征表示。

**补充：**掩码自注意力机制即每个文本Token仅与该Token之前的Token进行注意力机制计算，以此来保持文本的因果性。
{: .notice--info}

**关于一些训练参数和指标：**所有模型都训练32个epoch，使用Adam优化器，使用余弦学习率衰减调度，对所有不是 gain 或 bias 的权重施加 decoupled weight decay，初始超参数通过在训练 1 个 epoch 的 ResNet-50 基线模型上进行网格搜索、随机搜索和手动调参确定。mini-batch大小为32768。最大的 ResNet 模型 RN50x64 在 592 张 V100 GPU 上训练了 18 天，而最大的 Vision Transformer 模型则在 256 张 V100 上训练了 12 天。**对于 ViT-L/14，额外用 336 像素分辨率进行了 1 个 epoch 的预训练以提升性能。该模型被标记为 ViT-L/14@336px。这个图像编码器的表现最佳。**

## 实验分析

### 文本提示方面的提升

首先是**提示工程（Prompt Engineering）**，即为大模型设计、优化、组合提示词，来驱动模型产生更好结果。在CLIP中，作者发现使用一句话来作为文本提示要好于使用一个单词，并且对不同任务使用不同句式会更合适。例如在ImageNet中使用句式“A photo of a {label}.”；在Oxford-IIIT Pets明确类别类型：“A photo of a {label}, a type of pet."；在卫星图像分类数据集上使用"A satellite photo of a {label}."。

然后是**集成（Ensembling）**，即对同一类类别使用多个相似的文本提示，然后将这些文本的编码进行平均集成，作为这个类的最终类别向量，这么做向量信息会更丰富，更加鲁棒。例如将“A photo of a cat.”、“A photo of a small cat.”、“A photo of a big cat.”和“A photo of a cat, a type of pet.”的文本向量集成后作为cat这个类的文本提示。

### CLIP擅长和不擅长的数据集类型

为了探究CLIP在不同数据集上的零样本迁移能力，作者对比了zero-shot CLIP和完全监督训练模型ResNet50在不同数据集上的准确度，具体结果如下，图中数值表示CLIP相对于全监督模型ResNet50：

<img src="/assets/images/2025-11-18/0002.png" alt="CLIP与全监督模型对比" style="zoom:50%;" />{: .align-center}

其中各个数据集的类型如下表所示：

| **CLIP 表现好**  | **任务类型 / 图像内容简介**            | **CLIP 表现差** | **任务类型 / 图像内容简介**          |
| ---------------- | -------------------------------------- | --------------- | ------------------------------------ |
| Stanford Cars    | 细粒度汽车分类，不同品牌型号的汽车图片 | Birdsnap        | 细粒度鸟类分类，近 500 种鸟类的照片  |
| Country211       | 国家分类，城市景观、地标、文化相关图片 | MNIST           | 手写数字识别，28×28 黑白数字图片     |
| Food101          | 食物分类，101 种食物图片               | FGVCAircraft    | 细粒度飞机型号分类，飞机照片         |
| Kinetics700      | 视频动作识别，700 类动作               | **RESISC45**    | **遥感图像分类，卫星俯视场景**       |
| SST2             | 文本情感分类，电影评论短句             | Flowers102      | 细粒度花朵分类，102 种花照片         |
| SUN397           | 场景分类，397 种室内/室外场景          | DTD             | 纹理分类，各类材质纹理图片           |
| UCF101           | 视频动作识别，101 类动作               | CLEVRCounts     | 合成图像计数任务，几何体场景         |
| Hateful Memes    | 图文理解，识别带仇恨含义的 meme        | GTSRB           | 交通标志分类，德国路面交通标志       |
| STL10            | 图像分类，10 类通用物体                | PatchCamelyon   | 医学图像分类，组织切片癌细胞检测     |
| FER2013          | 面部表情识别，7 种表情                 | KITTI Distance  | 自动驾驶场景物体距离估计，街景图片   |
| Caltech101       | 多类物体分类，101 类常见物体           | **EuroSAT**     | **遥感土地覆盖分类，卫星多光谱图像** |
| PascalVOC2007    | 多类物体分类，20 类常见物体            |                 |                                      |
| Oxford-IIIT Pets | 细粒度宠物分类，37 种猫狗品种          |                 |                                      |
| CIFAR10/100      | 图像分类，10 类通用物体                |                 |                                      |
| ImageNet         | 图像分类，1000 类通用物体              |                 |                                      |

作者表示在一些专业化、复杂或抽象的任务上表现较弱，主要原因在于用来训练CLIP的数据集WIT没有与这些数据集类似的数据分布，导致零样本迁移能力要弱一些。

### 零样本迁移与少样本迁移比对

为了让CLIP更好的应用于下游任务，作者探究了零样本迁移的CLIP性能与进行少样本迁移（即使用少量样本进行微调）的模型性能对比。

<img src="/assets/images/2025-11-18/0003.png" alt="zero-shot与few-shot对比" style="zoom:50%;" />{: .align-center}

首先是直接对比零样本迁移CLIP和使用少量样本微调线性分类器的对比。如图所示，零样本迁移CLIP在性能上与4-shot的CLIP相当，比其他模型的16-shot还要好。

**补充：**few-shot linear probes是指冻结图像特征提取的网络部分，仅仅用少量样本训练模型最后的线性分类器。
{: .notice--info}

然后作者也展示了在不同数据集上每类需要多少样本来进行微调才能与零样本迁移性能相当，在各个数据集上需求差异较大，从0到上百张不等。另外，也展示了CLIP的零样本迁移性能仍然与全监督训练性能有一定差距（10%-25%）。

<img src="/assets/images/2025-11-18/0004.png" alt="不同数据集需求；zero-shot与few-shot对比" style="zoom:50%;" />{: .align-center}

### 基于线性分类器训练的模型对比

作者指出**端到端的微调**通常会比**训练线性分类器**效果好，但是微调可能会掩盖模型特征提取的缺陷，并且训练成本较高，因此作者选择了基于线性分类器训练的对比，即针对不同的下游任务对分类器进行训练，然后对比当时的SOTA模型，具体结果如下：

![CLIP对比结果](/assets/images/2025-11-18/0005.png)
